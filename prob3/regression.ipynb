{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XizLhEXV4MKE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT DELETE ##\n",
        "\n",
        "def generate_linear_data(n:int, d:int, sigma:float):\n",
        "  '''\n",
        "    Input:\n",
        "      n: number of datapoints\n",
        "      d: dimension of features\n",
        "      sigma: std for gaussian noise for the synthetic linear data\n",
        "    returns 3 variables\n",
        "      X the design matrix shape is (n x d) each element in the matrix is in [0, 1]\n",
        "      y the associated labels shape is (n,) each y is in [-1, 1]\n",
        "      theta is randomly generated on the surface of the sphere,\n",
        "  '''\n",
        "  X = np.random.randn(n, d) # entries uniform in [0,1), thus each row ||x_i|| bounded by 1\n",
        "  theta = np.random.normal(0, 1, d) # d iid Gaussian each entry N(0, 1)\n",
        "  theta = theta / np.linalg.norm(theta) # theta is uniformly distributed on the surface of unit sphere, shape (d,)\n",
        "  y = (X @ theta) + np.random.normal(0, sigma, size=n) # so essentially the learner is trying to recover \\theta\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "jWUsXcFR4U97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT DELETE ##\n",
        "def mse_loss(y_true:np.ndarray, y_pred:np.ndarray):\n",
        "  '''\n",
        "  Inputs:\n",
        "    y_true : the ground truth labels vector\n",
        "    y_pred : the predicted labels vector\n",
        "  '''\n",
        "  assert len(y_true) == len(y_pred) # assert that the size is same\n",
        "\n",
        "  return mean_squared_error(y_true, y_pred) # MSE calculation"
      ],
      "metadata": {
        "id": "iwJ0fbr546wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_regression_soln(X_train:np.ndarray, y_train:np.ndarray, \\\n",
        "                           X_test:np.ndarray, y_test:np.ndarray) -> np.ndarray:\n",
        "    '''\n",
        "     TODO complete this function ;\n",
        "     Implement linear regression and compute the mse (3 PTS)\n",
        "\n",
        "     Returns:\n",
        "        mse_loss_linear_reg (float) : the mean squared error on the test set\n",
        "    '''\n",
        "\n",
        "    # TODO Implement this function\n",
        "\n",
        "    # return mse_loss_linear_reg"
      ],
      "metadata": {
        "id": "i_MmYMI35Z3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ridge_regression_soln(X_train:np.ndarray, y_train:np.ndarray, \\\n",
        "                        X_test:np.ndarray, y_test:np.ndarray, \\\n",
        "                        lamb:float) -> np.ndarray:\n",
        "    '''\n",
        "     TODO complete this function ;\n",
        "     Implement ridge regression and compute the mse (3 PTS)\n",
        "\n",
        "    HINT --- Solve this in three steps\n",
        "\n",
        "    Step (1) -- Compute the optimal theta vector on the train set (call it `theta_opt`)\n",
        "    This is a (np.ndarray) of size d (d = number of features.\n",
        "\n",
        "    Step (2) -- Compute the prediction vector using the above on the test set and return this prediction vector.\n",
        "\n",
        "    Step (3) -- Compute MSE (see the corresponding function in utils.py file) and return the MSE value\n",
        "\n",
        "    Returns:\n",
        "        mse_loss_ridge_reg (float) : the mean squared error on the test set\n",
        "    '''\n",
        "\n",
        "    # TODO Implement this function\n",
        "\n",
        "    # return mse_loss_ridge_reg"
      ],
      "metadata": {
        "id": "-yaYLy8l4U75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT CHANGE THE BELOW LINES !\n",
        "\n",
        "X, y = generate_linear_data(n = 1000, d = 20, sigma = 0.0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 37)\n",
        "# N_train, N_test = len(X_train), len(X_test)\n",
        "\n",
        "mse_loss_linear_reg = linear_regression_soln(X_train, y_train, X_test, y_test)\n",
        "print(mse_loss_linear_reg) # -- Sanity check; this value should be very close to 0, think why!\n",
        "\n",
        "lambda_list = [0.1, 0.5, 1, 5, 10]\n",
        "mse_list = []\n",
        "\n",
        "for lamb in lambda_list:\n",
        "\n",
        "    mse_loss_ridge_reg = ridge_regression_soln(X_train, y_train, X_test, y_test, lamb)\n",
        "    mse_list.append(mse_loss_ridge_reg)\n",
        "\n",
        "print(mse_list)"
      ],
      "metadata": {
        "id": "Cp4MwxEF4t6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO plot the mse values (`mse_list`) on the y-axis and the lambda values on the x-axis for ridge regression\n",
        "# use matplotlib"
      ],
      "metadata": {
        "id": "ynb9OYy74U5e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}