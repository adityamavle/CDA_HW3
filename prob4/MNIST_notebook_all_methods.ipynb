{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "Gv5990jj4Jvg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b4372d02d0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "torch.manual_seed(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8-PUZHj-74u"
   },
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "uH2Xr_EH6IMj"
   },
   "outputs": [],
   "source": [
    "## DO NOT DELETE\n",
    "# HYPERPARAMETERS\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 5\n",
    "C = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfyemgpu-6bK"
   },
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "mKDHLjtk785L"
   },
   "outputs": [],
   "source": [
    "## DO NOT DELETE\n",
    "# PREPROCESSING\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "SBCf9ukCDDeo"
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE ###\n",
    "def evaluate(model, data_loader, method=None):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            if method == 'lr':\n",
    "              outputs = model(data.view(-1,28*28))\n",
    "            else:\n",
    "              outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEmhH_It19xB"
   },
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, target = next(iter(train_loader))\n",
    "\n",
    "data.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "TrqN8qSM1_x9"
   },
   "outputs": [],
   "source": [
    "## DO NOT DELETE\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        # TODO implement LR model here\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)  \n",
    "    def forward(self, x):\n",
    "        # TODO implement forward pass here\n",
    "        x = x.view(x.size(0), -1) #Flatten the input tensor\n",
    "        z = self.linear(x) #linear transformation\n",
    "        return z        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "qi2YWmjoIOju"
   },
   "outputs": [],
   "source": [
    "## DO NOT DELETE\n",
    "lr_model = LogisticRegression(input_size= 28 * 28 , num_classes= 10)\n",
    "loss_fn = nn.CrossEntropyLoss() #multiclass log softmax function\n",
    "lr_optimizer = torch.optim.Adam(lr_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "17BdIH56IOfm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:08<00:33,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, NN Loss: 0.3909980665916192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:16<00:24,  8.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, NN Loss: 0.2937971075007847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:24<00:16,  8.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, NN Loss: 0.28207019655339755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:33<00:08,  8.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, NN Loss: 0.2751435860673756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:42<00:00,  8.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, NN Loss: 0.27088627276787247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lr_losses = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    lr_model.train()\n",
    "    lr_loss_per_epoch = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Train LR model\n",
    "        # TODO Implement the lr model training loop here\n",
    "        pred = lr_model(data) #X = data\n",
    "        loss = loss_fn(pred, target)\n",
    "\n",
    "        # loss.backward()\n",
    "        lr_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        lr_optimizer.step()\n",
    "        lr_loss_per_epoch += loss.item()\n",
    "\n",
    "    lr_losses.append(lr_loss_per_epoch / len(train_loader))\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, NN Loss: {lr_losses[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "JoawpAojIOaf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.38"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement the LR evaluation code\n",
    "# Hint : use the `evaluate` function above on the test set\n",
    "evaluate(lr_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fwfe1cGx0v0U"
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "Jzv5CScp8ExO"
   },
   "outputs": [],
   "source": [
    "## DO NOT DELETE\n",
    "class SVM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        # TODO implement SVM model here\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes, bias = False)\n",
    "        nn.LogSigmoid\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO implement the forward pass here\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "0vDm8Dzt8Xe9"
   },
   "outputs": [],
   "source": [
    "## DO NOT DELETE\n",
    "# SVM MODEL AND OPTIMIZER\n",
    "svm_model = SVM(input_size= 28 * 28, num_classes= 10)\n",
    "svm_optimizer = optim.SGD(svm_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "ivoYB4M18mcg"
   },
   "outputs": [],
   "source": [
    "## DO NOT DELETE\n",
    "# HINGE LOSS FOR SVM\n",
    "def hinge_loss(outputs, labels):\n",
    "    num_samples = outputs.size(0)\n",
    "    correct_scores = outputs[torch.arange(num_samples), labels].unsqueeze(1)\n",
    "    margins = torch.clamp(1 - (correct_scores - outputs), min=0)\n",
    "    margins[torch.arange(num_samples), labels] = 0\n",
    "    loss = torch.sum(margins) / num_samples\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "VcMJXtGa6IKb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:07<00:29,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, SVM Loss: 0.7875140408780783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:14<00:21,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, SVM Loss: 0.5038530819340429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:22<00:14,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, SVM Loss: 0.4608617932525779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:29<00:07,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, SVM Loss: 0.4379785778854829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:37<00:00,  7.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, SVM Loss: 0.4235363910034267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "svm_losses, ls_svm_losses = [], []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    svm_model.train()\n",
    "    svm_loss_per_epoch = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Train SVM\n",
    "        # TODO Implement the SVM model training loop\n",
    "\n",
    "        pred = svm_model(data)\n",
    "        loss = hinge_loss(pred, target) #calculating the SVM hinge loss\n",
    "\n",
    "        svm_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        svm_optimizer.step()\n",
    "        svm_loss_per_epoch  += loss.item()\n",
    "        \n",
    "    svm_losses.append(svm_loss_per_epoch / len(train_loader))\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, SVM Loss: {svm_losses[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "ZMfPkhSg6IIO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.48"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement the SVM evaluation code\n",
    "# Hint : use the `evaluate` function above on the test set\n",
    "evaluate(svm_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_GPG_Qw2GaR"
   },
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "G_-h4onw2Iw0"
   },
   "outputs": [],
   "source": [
    "## DO NOT DELETE\n",
    "class NN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, num_classes, hidden_size = 128):\n",
    "        # TODO implement NN model here\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO implement forward pass here\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = torch.relu(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "dlJOSLez2Is9"
   },
   "outputs": [],
   "source": [
    "## DO NOT DELETE\n",
    "nn_model = NN(input_size = 28 * 28, num_classes= 10)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "nn_optimizer = torch.optim.Adam(nn_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "ZiheaTfg2zFs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:08<00:33,  8.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, NN Loss: 0.26038076001793337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:17<00:25,  8.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, NN Loss: 0.11336344412502164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:25<00:16,  8.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, NN Loss: 0.0798286733960709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:34<00:08,  8.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, NN Loss: 0.06089003459820504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:42<00:00,  8.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, NN Loss: 0.04836411845918967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nn_losses = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    nn_model.train()\n",
    "    nn_loss_per_epoch = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Train NN model\n",
    "        # TODO Implement the nn model training loop\n",
    "        pred = nn_model(data)\n",
    "\n",
    "        loss= loss_fn(pred, target)\n",
    "\n",
    "        nn_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn_optimizer.step()\n",
    "\n",
    "        nn_loss_per_epoch += loss.item()\n",
    "    nn_losses.append(nn_loss_per_epoch / len(train_loader))\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, NN Loss: {nn_losses[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "5m_pq5ezDLlX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.46"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement the NN evaluation code\n",
    "# Hint : use the `evaluate` function above on the test set\n",
    "evaluate(nn_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYTEEKTODxfP"
   },
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "Zjcu2UNLDz9G"
   },
   "outputs": [],
   "source": [
    "## DO NOT DELETE\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        # TODO implement CNN model here\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO implement forward pass here\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "Xldya81jFhIp"
   },
   "outputs": [],
   "source": [
    "## DO NOT DELETE\n",
    "cnn_model = CNN(num_classes= 10)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "cnn_optimizer = torch.optim.Adam(cnn_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "U0uLpugRFhIp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:10<00:41, 10.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, CNN Loss: 0.1789715369353627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:20<00:30, 10.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, CNN Loss: 0.05470071148463904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:30<00:20, 10.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, CNN Loss: 0.04111397223396183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:40<00:10, 10.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, CNN Loss: 0.033800749572305215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:50<00:00, 10.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, CNN Loss: 0.027818138798062594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_losses = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    cnn_model.train()\n",
    "    cnn_loss_per_epoch = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Train CNN model\n",
    "        # TODO Implement the nn model training loop\n",
    "        cnn_optimizer.zero_grad()\n",
    "        pred = cnn_model(data)\n",
    "        loss = loss_fn(pred, target)\n",
    "        loss.backward()\n",
    "        cnn_optimizer.step()\n",
    "        cnn_loss_per_epoch += loss.item()\n",
    "    cnn_losses.append(cnn_loss_per_epoch / len(train_loader))\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, CNN Loss: {cnn_losses[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "Ky4g2_9hFhIp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.6"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement the CNN evaluation code\n",
    "# Hint : use the `evaluate` function above on the test set\n",
    "evaluate(cnn_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
