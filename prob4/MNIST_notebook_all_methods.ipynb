{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gv5990jj4Jvg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "torch.manual_seed(21)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters"
      ],
      "metadata": {
        "id": "q8-PUZHj-74u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT DELETE\n",
        "# HYPERPARAMETERS\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 5\n",
        "C = 1.0"
      ],
      "metadata": {
        "id": "uH2Xr_EH6IMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "xfyemgpu-6bK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT DELETE\n",
        "# PREPROCESSING\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "mKDHLjtk785L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### DO NOT DELETE ###\n",
        "def evaluate(model, data_loader, method=None):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            if method == 'lr':\n",
        "              outputs = model(data.view(-1,28*28))\n",
        "            else:\n",
        "              outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "    return 100 * correct / total"
      ],
      "metadata": {
        "id": "SBCf9ukCDDeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "MEmhH_It19xB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT DELETE\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        # TODO implement LR model here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO implement forward pass here\n",
        "        return"
      ],
      "metadata": {
        "id": "TrqN8qSM1_x9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT DELETE\n",
        "lr_model = LogisticRegression()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "lr_optimizer = torch.optim.Adam(lr_model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "qi2YWmjoIOju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_losses = []\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    lr_model.train()\n",
        "    lr_loss_per_epoch = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Train LR model\n",
        "        # TODO Implement the lr model training loop here\n",
        "    lr_losses.append(lr_loss_per_epoch / len(train_loader))\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, NN Loss: {lr_losses[-1]}')"
      ],
      "metadata": {
        "id": "17BdIH56IOfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Implement the LR evaluation code\n",
        "# Hint : use the `evaluate` function above on the test set"
      ],
      "metadata": {
        "id": "JoawpAojIOaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM"
      ],
      "metadata": {
        "id": "Fwfe1cGx0v0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT DELETE\n",
        "class SVM(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        # TODO implement SVM model here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO implement the forward pass here\n",
        "        return"
      ],
      "metadata": {
        "id": "Jzv5CScp8ExO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT DELETE\n",
        "# SVM MODEL AND OPTIMIZER\n",
        "svm_model = SVM()\n",
        "svm_optimizer = optim.SGD(svm_model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "0vDm8Dzt8Xe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT DELETE\n",
        "# HINGE LOSS FOR SVM\n",
        "def hinge_loss(outputs, labels):\n",
        "    num_samples = outputs.size(0)\n",
        "    correct_scores = outputs[torch.arange(num_samples), labels].unsqueeze(1)\n",
        "    margins = torch.clamp(1 - (correct_scores - outputs), min=0)\n",
        "    margins[torch.arange(num_samples), labels] = 0\n",
        "    loss = torch.sum(margins) / num_samples\n",
        "    return loss"
      ],
      "metadata": {
        "id": "ivoYB4M18mcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_losses, ls_svm_losses = [], []\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    svm_model.train()\n",
        "    svm_loss_per_epoch = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Train SVM\n",
        "        # TODO Implement the SVM model training loop\n",
        "    svm_losses.append(svm_loss_per_epoch / len(train_loader))\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, SVM Loss: {svm_losses[-1]}')"
      ],
      "metadata": {
        "id": "VcMJXtGa6IKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Implement the SVM evaluation code\n",
        "# Hint : use the `evaluate` function above on the test set"
      ],
      "metadata": {
        "id": "ZMfPkhSg6IIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NN"
      ],
      "metadata": {
        "id": "4_GPG_Qw2GaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT DELETE\n",
        "class NN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        # TODO implement NN model here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO implement forward pass here\n",
        "        return"
      ],
      "metadata": {
        "id": "G_-h4onw2Iw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT DELETE\n",
        "nn_model = NN()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "nn_optimizer = torch.optim.Adam(nn_model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "dlJOSLez2Is9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_losses = []\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    nn_model.train()\n",
        "    nn_loss_per_epoch = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Train NN model\n",
        "        # TODO Implement the nn model training loop\n",
        "    nn_losses.append(nn_loss_per_epoch / len(train_loader))\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, NN Loss: {nn_losses[-1]}')"
      ],
      "metadata": {
        "id": "ZiheaTfg2zFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Implement the NN evaluation code\n",
        "# Hint : use the `evaluate` function above on the test set"
      ],
      "metadata": {
        "id": "5m_pq5ezDLlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "ZYTEEKTODxfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT DELETE\n",
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        # TODO implement CNN model here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO implement forward pass here\n",
        "        return"
      ],
      "metadata": {
        "id": "Zjcu2UNLDz9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT DELETE\n",
        "cnn_model = CNN()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "cnn_optimizer = torch.optim.Adam(cnn_model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "Xldya81jFhIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_losses = []\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    cnn_model.train()\n",
        "    cnn_loss_per_epoch = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Train CNN model\n",
        "        # TODO Implement the nn model training loop\n",
        "    cnn_losses.append(cnn_loss_per_epoch / len(train_loader))\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, CNN Loss: {cnn_losses[-1]}')"
      ],
      "metadata": {
        "id": "U0uLpugRFhIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Implement the CNN evaluation code\n",
        "# Hint : use the `evaluate` function above on the test set"
      ],
      "metadata": {
        "id": "Ky4g2_9hFhIp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}